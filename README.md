
# 新闻题材中人物-言论自动提取

项目主要展示部分包括了：新闻人物言论提取、人物社会关系知识图谱、新闻实体抽取后的词云展示。同时搭建Flask+Bootstrap前后台完成相关功能展示。

## 主要功能描述

**1.新闻人物言论提取**

+ **句段预处理** ：以引号(“”)，句号(。)，感叹号(!)，问号(？)为基准对句子进行分割获得片段，
+ **分析句式结构** 获得句子后，依照预设的特征将原句进行分割，确定发言人所在句段以及言论所在句段。 注：发言人所在句段经常同时存在“发言人”和“言论”。
+ **信息抽取** 通过**依存分析**来解析句段结构。以“说”(或其他预设动词)为界，对左搜寻‘发言人’，对右搜寻‘言论’的起始点。
+ **言论合并** 获得检索结果后，基于**句子向量**来分析是否应将该言论与其余言论合并

**2.人物社会关系知识图谱**

+ **抽取实体**，判断句子中是否存在实体，以进行下一步分析、
+ **抽取人物实体修饰部分**，并确认修饰部分是否存在组织、机构等实体
+ **合并实体**，确立人物社会关系

**3.新闻实体提取**

+ 为做词云效果展示，提取句子中人物、组织、机构等命名实体。

**4.前后台框架**

+ 前后台搭建使用Flask+Bootstrap+d3+mysql，同时使用了scrapy+mysql+redis实现增量去重采集人民网、新华网新闻数据，以提供新闻素材。

## 项目部署

项目部署至Ubuntu参考地址：https://www.digitalocean.com/community/tutorials/how-to-serve-flask-applications-with-uswgi-and-nginx-on-ubuntu-18-04

项目启动文件wsgi.py中from run import app,在测试过程中无法正常使用，按照网上意见改成from run imort app as application,同时下方的app.run()改成application.run

ltp模型为哈工大的语言模型，model自己训练模型。训练方法在/app/controller/Pretreatment.py中，分别使用ltp与model文件夹分别放在与run.py同级目录

使用github中webhooks对服务器端代码进行自动更新，代码提交到github上，服务器自动更新并重启服务

## 经验教训

+ **哈工大语言模型安装：**尝试用python3.7版本安装未成功，后来发现平台最高支持3.6版本。  
安装过程：https://blog.csdn.net/laoyaotask/article/details/45312905 （原文出处找不到了）
+ **哈工大与jieba分词进行比较：**在Model.py文件中使用jieba_compare_pyltp()临时函数对两个工具交叉功能进行比较，发现pyltp分词效果好于jieba，但缺点是执行效率慢。
+ **算法模型执行速度慢：**在打印每个函数执行时间的过程中发现get_count()函数执行次数非常多，它主要为实现Stanford的句子相似度算法，计算句子中每个词语的词频及词向量，每篇文章都要重复调用几十次以上。同时，发现哈工大的分词工具pyltp.segment执行每次在3秒以上，相当耗时。
+ **普林斯顿语句近似度算法**：按照论文算法只完成了Vs=1/|s|*∑a/(a+p(w))*Vw，**后半部分的矩阵运算不知道该用如何工具去运算**。
